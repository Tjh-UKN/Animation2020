# 基于神经网络的人物动作捕捉和角色动作生成文献综述
## 1 背景




## 2 里程碑

### 2.1 相位函数神经网络(PFNN)

### 2.2 模式自适应神经网络(MANN)

四足动物动画是计算机动画中一个未解决的关键问题。它不仅同计算机游戏和电影的实现有关，也是机器人学中一个具有挑战性的主题。在控制数据获取中，四足动物不能像人类一样被引导，这是一大难题。（就意味着动作的数据样本不是按人为意愿控制的）所以获得的数据经常是无序的，连续动作也是随机的。
混合专家模式是一个传统的机器学习方法，大量的experts被用来处理不同地带的输入。一个门网络决定了对于给定的输入，哪些专家会被使用。训练完成后，这些专家会专用于被门网络分配的下降维度。它和深度学习结构的组合显示出极好的前景。基于传统机器学习技术的动作合成：如K-Nearnest Neighbours（KNN），主成分分析(PCA)，径向基数函数(RBF)，强化学习，高斯处理(GP) 等深度学习技术用于改善不同动作的过渡以及提升输出运动的普适性，最好是生成全新的动作而不是仅仅模仿已有的动作数据。2018年HE ZHANG†， SEBASTIAN STARKE基于神经网络学习和混合专家模型提出了MANN神经网络（四足动物运动控制中的模态自适应神经网络），用于四足动物的动作生成和预测。
该系统由门控网络和动作预测网络组成，门控网络负责接收动作的特征x和混合expert weights动态更新预测网络权重，（混合专家模式是一个传统的机器学习方法，大量的experts被用来处理不同地带的输入。一个门网络决定了对于给定的输入，哪些专家会被使用。训练完成后，这些专家会专用于被门网络分配的下降维度。它和深度学习结构的组合显示出极好的前景。）动作预测网络负责计算当前帧的状态（即上一个动作的下一个状态），所用到的权重由门控网络计算。

MANN的贡献在以下几个方面：
1.第一个系统性的数据驱动的生成高质量复杂四足动作模式以及转换的角色控制器。
2.灵活性高，这个系统可以通过无结构的动作捕获数据，持续学习一系列周期和非周期的动作的expert weight，同时它不需要相位标记和步态标记。


### 2.3 神经状态机(NSM)

基于场景理解的精确控制能够很好地帮助人物角色实现自我定位和导航，从而到达设定的目标位置。Holden等人提出了神经状态机(NSM)，一种引导角色通过精确场景交互实现目标驱动行为的框架。Holden等人在2017年提出的相位函数神经网络(PFNN)通过监督学习进行建模，生成高质量的人物动作并控制角色在有限区域内的运动。神经状态机将高质量的动作生成应用于支持人物角色与环境的交互，基于高阶的场景理解控制角色实现一系列复杂的人物动作。

神经状态机的贡献主要体现在以下几点：
1. 神经状态机构造出一种信号，将动作的相位编码成为高级动作描述标签和目标位置，通过数据集的深度学习，神经网络能够根据高级的指令生成高质量的人物角色动作。神经状态机以一种端到端的方式从数据集中分解权重，改良了传统的使用固定相位函数方法所带来的局限性，即只能应用于周期性的人物动作。
2. 神经状态机实现了一种双向控制框架，该框架综合了第一人称视角和目标视角对人物角色动作的预测，预测结果被实时地反馈给神经网络，从而产生连续的、高精确度的角色运动轨迹。
3. 神经状态机将体积表示方法应用于环境理解，改良了传统等高线表示方法所带来的局限性。该方法增强了了角色与凹形物体的交互效果，
4. 作者设计了一种增强数据集的方案，通过在数据集的每一帧中随机地切换环境变量中的几何物体，且不改变动作和交互的连贯性，使得神经网络获得更大的学习量，而不需要增大数据集的数据量。



## 3 相关工作



### 实现角色与环境动态交互的技术

**1.1 基于运动学的动作序列合成**

**基于模板的方法**将经过剪辑的运动片段插入场景中，并依据场景进行适应性调节。这些方法易于实现，但无法为大规模动画生成连续的运动数据。

1. Kang Hoon Lee等人于2006年提出了使用积木块作为“动作补丁”，构建大面积的虚拟环境，引导角色寻找达到目标位置的方法。积木块被赋予动作数据，包含了角色所被允许进行交互的动作集合。动作补丁模型实现了在较为复杂的环境中生成角色的动作。
2. Shailen Agrawal等人于2016年提出了一种目标驱动的运动模型，该法将人物角色的几种不同类型的脚步作为模板，通过调用和优化角色的脚步计划，并以此为基础生成人物角色全身的运动。该法实现了人物角色与环境之间更密切的交互。

**基于内核的方法**解决了基于模板的方法留下的问题。

1. Tomohiko Mukai等人为了解决地质统计学的相关问题，提出了将插入的动作片段视为参数空间内的数据预测的方法。
2. Jack M. Wang等人于2008年提出了高斯过程的动力学模型(GPDMs)，并将其应用于通过高维运动捕捉数据学习人类角色的姿势和动作。该类模型包含与动力学相关的低维隐式空间，以及从隐式空间到显式空间的映射。

**1.2 通过搜索的运动合成**

1. Min Gyu Choi等人于2003年提出了环境中的概率路线图，并且使用动捕数据和概率路线图生成二足动物角色的动作。这套方案以概率路径规划和多层次位移映射为基础，包括路线图构建、路线图搜索和运动合成三个部分。
2. Wan-Yen Lo和Matthias Zwicker 于2003年将强化学习应用于引导人物角色穿过门，于2012年实现引导人物角色绕开障碍物到达目的地。
3. Safonova和Hodgins于2007年引入A*搜索算法，用于混合动作插值的权重从而产生一个新的运动。此模型相当于一个参数化的图结构。Jianyuan Min等人使用最大后验估计(MAP)优化了动作权重的混合过程。

**1.3 通过局部优化或随机优化合成运动**。Igor Mordatch等人以一种局部优化策略——接触不变的优化方法为内核，提出了一种动作合成框架。Wenping Zhao等人使用一种随机优化策略——微粒群优化算法生成人物抓取的动作。

### 应用于合成密切交互的深度学习技术

**2.1 深度监督学习技术**

1. Katerina Fragkiadaki等人在2015年将长短期记忆神经网络( Long Short Term Memory Networks, LSTMs)应用于角色移动的动画模拟；Zimo Li、Ruben Villegas等人的团队将该技术应用于更为复杂的人物动作的刻画。
2. Zimo Li等人于2017年通过自适应的递归神经网络(auto-conditioned Recurrent Neural Network, RNN)的Teacher Forcing训练机制，改善了LSTMs随着错误的积累模拟精确度逐渐下降的问题。
3. Shaojie Bai等人于2018年指出使用简单的卷积神经网络结构进行模拟的效果优于RNN和LSTMs。
4. Daniel Holden等人于2016年将时间卷积应用于这一领域的研究，并于2017年提出相位函数神经网络(PFNN)，用以进行人物角色运动的生成。
5. Robert A. Jacobs在1991年提出的多专家模型(the mixture of experts)是一种监督学习模型。以此为基础，He Zhang等人于2018年提出模式自适应神经网络(mode-adaptive neural network, MANN)，用以模拟四足动物的运动。该模型由预测神经网络和一个门网络组成。

**2.2 深度强化学习技术**

1. 深度强化学习被广泛应用于人物动作的模拟。Xue Bin Peng等人于2018年前后引入参考数据(reference data)和视频数据进行训练，弥补了由单一的数据反馈带来的不足。
2. Wenhao Yu等人于2018年提出了一种训练对称和节能的运动模式的方法。通过为损失函数引入不对称的惩罚项，以及使用额外的辅助学习(locomotion curriculum learning)，生成了更符合人类认知的行走动作。

### 基于物理的动作生成技术
**3.1 基于轨迹的方法**

吉拉德和Maciejewski早期工作[1985]建议使用的步态模式，脚的位置样条，逆运动学，和身体的位置，简化体动力学的制约。
布隆伯格和Galyean[1995]培养作为模拟一个多层的运动方法电机系统的狗，重点支持更高层次的行为。
[海克尔等，2008]游戏的发展产生任意足动物，包括运动模式的程序动画的方法。
Torkos和VAN+DE+Panne[1998]轨迹优化技术应用于一个抽象的quadrupedmodel获得与脚位置和计时模式兼容的议案。
Wampler和Popovic通过最优化扭转力和步态循环中约束条件的组合的方式计算不同类型的的动物的动作。
W扩展了这一方法，并用来预测外表已知而动作未知的动物的行动。这些方法不能应用到在线性应用上。
Levine和Popovic以类似物理的方式让狗的运动适应动态改变的地形。这种方法对使少量的典型动作适应不同环境很有用，但需要一系列动作作为先导，这个方法发展出了由数据驱动的方法。

**3.2 基于扭转力的方法**

Raibert和Hodgins设计出了小跑，反射和飞奔步态的控制策略。
Van和Panne用速度作为矩阵最优化控制图的参数来控制一个猫的模型。
Coros【Locomotion skills for simulated quadrupeds. ACM Transactions on Graphics 30, 4 (2011), 】为四足动物设计出一个细节控制器，（有一个灵活的抽象脊柱）通过最优化PD控制器在满足约束的同时来模仿视频中的动作。从而模拟出大量的动作包括走，小跑，慢跑，飞奔。
Peng在控制四足动物模型时应用了强化学习以适应不同的2d地面状态【2015】。
在高维状态空间设计一个稳定的控制器是一个很难的问题，因此低维空间特征需要被手动调控来保持控制器的稳定。Peng通过对自动计算出的特征空间应用深度强化学习解决了这一问题。这个方法又被提升于3d环境和应用到二足动物角色上【2016】。

**3.3 补充**

与基于物理的动画相对的是数据驱动的动画技术，使用捕获的运动数据来进行交互式的角色控制。数据结构比如动作图(motion graphs)，被用来在无序的动作捕获数据中生成连续角色动作。动作图的连通性影响了被控制的角色的反应度，计算机游戏和其他交互应用经常使用更简单的结构，比如有限状态机(finite state machine)它的连通性更加明显，并且序列性动作可以预测。

## 4 应用和展望

